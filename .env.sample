# LLM Configuration
# Copy this file to .env and fill in your actual values

# API Configuration
LLM_API_URL=https://api.example.com/v1/
LLM_API_KEY=your-api-key-here
LLM_MODEL=model-name-here

# Example configurations for different providers:

# For OpenAI:
# LLM_API_URL=https://api.openai.com/v1/
# LLM_API_KEY=sk-your-openai-api-key
# LLM_MODEL=gpt-4

# For Anthropic Claude:
# LLM_API_URL=https://api.anthropic.com/v1/
# LLM_API_KEY=sk-ant-your-anthropic-key
# LLM_MODEL=claude-3-opus-20240229

# For Local Llama/Ollama:
# LLM_API_URL=http://localhost:11434/v1/
# LLM_API_KEY=optional-or-leave-empty
# LLM_MODEL=llama2

# For Cerebras:
# LLM_API_URL=https://api.cerebras.ai/v1/
# LLM_API_KEY=csk-your-cerebras-key
# LLM_MODEL=openai/qwen-3-coder-480b

# Optional: Set to True to enable LiteLLM verbose logging
LITELLM_VERBOSE=False